{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yLVz2_-4fFr"
      },
      "source": [
        "# Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIOYbSM337lE"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ztrwV8-4Quc"
      },
      "outputs": [],
      "source": [
        "!pip install urduhack[tf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL27WiYl4k0o"
      },
      "source": [
        "# Importing modules and mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlVMoeCF4Kkq"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import keras\n",
        "import pickle\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import plot_model\n",
        "from google.colab import drive\n",
        "from urduhack.normalization import normalize_characters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import io\n",
        "import ast\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Dense, Embedding, GRU, Dropout, Bidirectional, SpatialDropout1D,TimeDistributed,LSTM"
      ],
      "metadata": {
        "id": "2sCOAvoAopwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CQYn4pi4bJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b8a833-d276-4d74-8fc5-823c4eb23cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Class"
      ],
      "metadata": {
        "id": "LwH1OsJkuCbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwqOlIXo4zzJ"
      },
      "outputs": [],
      "source": [
        "class CNN_subword_embeddings:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.max_len = 34\n",
        "    self.ft = fasttext.load_model('/content/drive/MyDrive/wiki.ur.bin')\n",
        "\n",
        "  def preprocessing(self,word):\n",
        "    return normalize_characters(u'{}'.format(word)).replace('\\u200c','').replace(' ','')\n",
        "\n",
        "  def get_fasttext_embeddings(self, word):\n",
        "    subwords = self.ft.get_subwords(word)[0]\n",
        "    embedding = [self.ft.get_word_vector(subword) for subword in subwords]  # getting embedding vectors from fasttext\n",
        "    return embedding\n",
        "\n",
        "  def pad_seq(self,fasttext):\n",
        "    if len(fasttext) < self.max_len:\n",
        "        fasttext+=[[0]*300]*(self.max_len - len(fasttext))\n",
        "    return fasttext\n",
        "\n",
        "  def get_model(self):\n",
        "  #m : number of words in a sentence\n",
        "    n = 150                                                   # number of filters\n",
        "    k = (1,2,3,4,5,6,7)                                       # kernel size of filters\n",
        "    emb_dim = 300                                             # embedding dimension\n",
        "    model = models.Sequential()\n",
        "    conv_layers = []                                          # different layers to run in parallel for different filter sizes\n",
        "    input_shape = layers.Input(shape=(self.max_len, emb_dim))\n",
        "    for kw in k:\n",
        "      c = layers.Conv1D(n, kw, activation=\"relu\")(input_shape)\n",
        "      conv_layers.append(layers.MaxPool1D(pool_size=self.max_len - kw +1)(c))\n",
        "    merged = layers.concatenate(conv_layers,axis=1)\n",
        "    merged = layers.Flatten()(merged)\n",
        "    out = layers.Dense(300)(merged)\n",
        "    model = models.Model(input_shape, out)\n",
        "    return model\n",
        "\n",
        "  def get_embedding(self,word):\n",
        "    word = self.preprocessing(word)\n",
        "    fasttext = self.get_fasttext_embeddings(word)\n",
        "    fasttext = self.pad_seq(fasttext)\n",
        "\n",
        "    x = np.stack([i for i in fasttext])\n",
        "    x = x.reshape(1,x.shape[0],x.shape[1])\n",
        "\n",
        "    model = self.get_model()\n",
        "    model.compile(optimizer=keras.optimizers.Adam(), loss='mean_squared_error', metrics=[keras.metrics.Accuracy()])\n",
        "\n",
        "    return tf.convert_to_tensor(model.predict(x)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN_subword_embeddings()\n",
        "word_ = cnn.get_embedding('شاداب')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fElHa0xo_Na",
        "outputId": "f5d232d3-22fc-4314-a1fb-a88d35f7f6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u_nlg9JpfXi",
        "outputId": "36738b87-98bf-4505-c9fe-f0714483d02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phonological Embeddings class"
      ],
      "metadata": {
        "id": "RifdFvvhQf16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Phonological_embeddings:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.splits=['e', 'o', 'a', 'i', 'u' ];\n",
        "    self.splits_b=['aa','ii','uu'];\n",
        "    self.splits_c=['e', 'o', 'i', 'u' ];\n",
        "    self.consonants=['ch','kh','sh','gh','bh','ph','th','jh','Th','chh','dh','ddh','rhh','b','p','t','j','s','d','z','r','f','k','g','l','m','n','h','w']\n",
        "    self.wordDict = pickle.load(open('/content/drive/MyDrive/word_dict.csv','rb'))\n",
        "  def get_subwords(self,token):\n",
        "    a = []\n",
        "    j = 0\n",
        "\n",
        "    if len(token) <= 3:\n",
        "      a.append([token])\n",
        "\n",
        "    elif len(token) == 4:\n",
        "      for i in range (0, len(token)-1):\n",
        "        if(token[i] in self.consonants and token[i+1] in self.consonants):\n",
        "          if(len(token[j:i+1])>1):\n",
        "              a.append([token[j:i+1]])\n",
        "              j=i+1\n",
        "\n",
        "        elif(token[i] in self.splits and token[i+1] in self.consonants):\n",
        "          if(len(token[j:i+1])>1):\n",
        "              a.append([token[j:i+1]])\n",
        "              j=i+1\n",
        "\n",
        "        else:\n",
        "          a.append([token[j:len(token)]])\n",
        "          j=i+2\n",
        "\n",
        "    else:\n",
        "      for i in range(0, len(token)):\n",
        "        if token[i:i+3] in self.consonants and i+3 < len(token):\n",
        "          if i+3 == len(token)-1:\n",
        "            if(len(token[j:i+4])>1):\n",
        "              a.append([token[j:i+4]])\n",
        "              j=i+4\n",
        "\n",
        "          else:\n",
        "            if(len(token[j:i+5])>1):\n",
        "              a.append([token[j:i+5]])\n",
        "              j=i+5\n",
        "\n",
        "        elif token[i:i+2] in self.splits_b and i+2 < len(token):\n",
        "          if token[i+2] in self.consonants and i+2 == len(token)-1:\n",
        "            if(len(token[j:i+3])>1):\n",
        "              a.append([token[j:i+3]])\n",
        "              j=i+3\n",
        "\n",
        "          else:\n",
        "            if(len(token[j:i+2])>1):\n",
        "              a.append([token[j:i+2]])\n",
        "              j=i+2\n",
        "\n",
        "        elif token[i] in self.splits:\n",
        "            if token[i+1:i+4] in self.consonants and i+5 == len(token):\n",
        "              if(len(token[j:i+1])>1):\n",
        "                a.append([token[j:i+1]])\n",
        "                j=i+1\n",
        "\n",
        "            elif token[i+1:i+3] in self.consonants:\n",
        "              if(len(token[j:i+3])>1):\n",
        "                a.append([token[j:i+3]])\n",
        "                j=i+3\n",
        "\n",
        "            elif token[i+1:i+2] in self.consonants and token[i+2:i+3] in self.consonants:\n",
        "              if(len(token[j:i+1])>1):\n",
        "                a.append([token[j:i+2]])\n",
        "                j=i+2\n",
        "\n",
        "            elif i+2 == len(token):\n",
        "              if(len(token[j:i+2])>1):\n",
        "                a.append([token[j:i+2]])\n",
        "                j=i+2\n",
        "\n",
        "            else:\n",
        "              if(len(token[j:i+1])>1):\n",
        "                a.append([token[j:i+1]])\n",
        "                j=i+1\n",
        "\n",
        "    a = [_ for __ in a for _ in __]\n",
        "    return a\n",
        "\n",
        "  def calculate_prosody(self,token):\n",
        "    n = []\n",
        "    b = []\n",
        "\n",
        "    for i in range(0, len(token)):\n",
        "      if len(token[i]) >= 3:\n",
        "        n.append(3)\n",
        "        b.append(110)\n",
        "\n",
        "      elif len(token[i]) == 1:\n",
        "        n.append(2)\n",
        "        b.append(10)\n",
        "\n",
        "      else:\n",
        "        if token[i][0:2] in self.splits_b:\n",
        "          n.append(3)\n",
        "          b.append(110)\n",
        "\n",
        "        elif ((token[i][0:1] in self.splits and token[i][1:2] in self.consonants) or (token[i][1:2] in self.splits and token[i][0:1] in self.consonants)) and i < len(token)-1:\n",
        "          n.append(3)\n",
        "          b.append(110)\n",
        "\n",
        "        else:\n",
        "          n.append(2)\n",
        "          b.append(10)\n",
        "\n",
        "    return [n, b]\n",
        "\n",
        "  def get_datavlues(self,token):\n",
        "    arr = []\n",
        "    num_pros = []\n",
        "    bin_pros = []\n",
        "\n",
        "    arr.append(self.get_subwords(token))\n",
        "    num_pros.append(self.calculate_prosody(arr[-1])[0])\n",
        "    bin_pros.append(self.calculate_prosody(arr[-1])[1])\n",
        "\n",
        "    arr = [e for sublist in arr for e in sublist]\n",
        "    num_pros = [e for sublist in num_pros for e in sublist]\n",
        "    bin_pros = [e for sublist in bin_pros for e in sublist]\n",
        "\n",
        "\n",
        "    return arr,num_pros,bin_pros\n",
        "\n",
        "  def encode_word(self,word):\n",
        "\n",
        "    token,num_pros,bin_pros=self.get_datavlues(word)\n",
        "    a = []\n",
        "\n",
        "    for t in token:\n",
        "      for j in range (0, len(self.wordDict)):\n",
        "        if t == self.wordDict[j][1]:\n",
        "          a.append(self.wordDict[j][0])\n",
        "\n",
        "    #padding\n",
        "    maxlen=5\n",
        "\n",
        "    if len(a)<maxlen:\n",
        "      a+=([list(self.wordDict)[-1]]*(maxlen-len(a)))\n",
        "\n",
        "    if len(num_pros) < maxlen:\n",
        "        num_pros += [0]*(maxlen-len(num_pros))\n",
        "    return a,num_pros\n",
        "\n",
        "  def return_model(self):\n",
        "    # EMBEDDING_DIM = 300\n",
        "    # vocab_size = list(self.wordDict.keys())[-1] + 2\n",
        "    # model = Sequential()\n",
        "\n",
        "    # model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=5))\n",
        "    # model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
        "\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    # model.add(TimeDistributed(Dense(4, activation='softmax')))\n",
        "\n",
        "    # model.compile(loss='categorical_crossentropy' , metrics=[keras.metrics.accuracy], optimizer=keras.optimizers.Adam())\n",
        "    model= phonetic = pickle.load(open('/content/drive/MyDrive/model.h5','rb'))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def get_embedding(self,word):\n",
        "    subwords,num_pros=self.encode_word(word)\n",
        "    model=self.return_model()\n",
        "    x_train = np.array(subwords).astype(np.float32)\n",
        "    intermediate_layer_model = keras.Model(inputs=model.input,\n",
        "                                       outputs=model.layers[0].output)\n",
        "    intermediate_output = intermediate_layer_model(x_train)\n",
        "    return intermediate_output\n"
      ],
      "metadata": {
        "id": "4oxdgk5rwhXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B/F LM CLass"
      ],
      "metadata": {
        "id": "QLGSkJPqroN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = pickle.load(open('/content/drive/MyDrive/labels_dict.sav','rb'))\n",
        "key = [key for key, val in label_dict.items() if val=='و']\n",
        "key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY1NeRHH3B6b",
        "outputId": "6beaf556-9188-4304-dd9f-68c601781521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3170]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bflm = BFLM()"
      ],
      "metadata": {
        "id": "slrCYukQaFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFLM:\n",
        "  def __init__(self):\n",
        "    self.max_len = 16\n",
        "    self.backward_model = tf.keras.models.load_model('/content/drive/MyDrive/backward_model130.h5')\n",
        "    self.forward_model = tf.keras.models.load_model('/content/drive/MyDrive/forward_model2.h5')\n",
        "    self.label_dict = pickle.load(open('/content/drive/MyDrive/labels_dict.sav','rb'))\n",
        "    # self.ytrain_labels = pickle.load(open('/content/drive/MyDrive/y_train_labels_dict.sav','rb'))\n",
        "\n",
        "  def pad_sequence(self, word_seq):\n",
        "    if len(word_seq)<self.max_len:\n",
        "      word_seq += [[0]*600]*(self.max_len - len(word_seq))\n",
        "      word_seq = [np.array(w) for w in word_seq]\n",
        "      return word_seq\n",
        "\n",
        "  def predict(self, word_seq, backward=False):\n",
        "    \"\"\"Expecting (len of seq, 600) word seq vector\"\"\"\n",
        "    # print(word_seq)\n",
        "    if backward:\n",
        "      model = self.backward_model\n",
        "    else:\n",
        "      model = self.forward_model\n",
        "    word_seq = self.pad_sequence(word_seq)\n",
        "    word_seq = np.array(word_seq)\n",
        "    # print(word_seq)\n",
        "    # print(word_seq.shape)\n",
        "    x = word_seq.reshape(1,word_seq.shape[0],word_seq.shape[1])\n",
        "    res = np.argmax(model.predict(x))\n",
        "    return self.label_dict[res]"
      ],
      "metadata": {
        "id": "Ki51_eu_rzjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drivers Code"
      ],
      "metadata": {
        "id": "lp1ZtDR7sY_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_subwords_together(phonological_embedding):\n",
        "  return phonological_embeddings[0]+phonological_embeddings[1]+phonological_embeddings[2]+phonological_embeddings[3]+phonological_embeddings[4]"
      ],
      "metadata": {
        "id": "6GZWXBJmvCj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urdu_to_roman = pickle.load(open('/content/drive/MyDrive/Urdu_to_roman.sav','rb'))\n"
      ],
      "metadata": {
        "id": "YPaZCLWSdAzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bflm = BFLM()"
      ],
      "metadata": {
        "id": "XXU1Ay00r1YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word = ['ہم', 'نشیں', 'مت', 'کہہ', 'کہ', 'برہم', 'کر', 'نہ', 'بزم', 'عیش', 'دوست', 'تو', 'میرے','غیر', 'ہے', 'قطع', 'لباس', 'خانہ','راز', 'نالہ', 'ہوں', 'کہ', 'بہ', 'شرح', 'نگاۂ','فہم', 'زنجیری', 'بے', 'ربطی', 'دل', 'نالے', 'کو', 'بھی', 'اعتبار', 'نغمہ', 'ہے']\n",
        "word = ['دوست']\n",
        "verses=[]\n",
        "for seed_word in word:\n",
        "  roman_word = urdu_to_roman[seed_word]\n",
        "  cnn = CNN_subword_embeddings()\n",
        "  cnn_embeddings = cnn.get_embedding(seed_word)\n",
        "\n",
        "  phn = Phonological_embeddings()\n",
        "  phonological_embeddings = phn.get_embedding(roman_word) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "  phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "  concat_vector = [tf.concat([cnn_embeddings,phonological_embeddings], axis=0).numpy().tolist()]\n",
        "  first_verse = [seed_word]\n",
        "\n",
        "  # Getting previous words\n",
        "  i = 0\n",
        "  while(True):\n",
        "    new_word = bflm.predict(list(concat_vector), backward=True)\n",
        "    i+=1\n",
        "    if new_word == '<SOS>' or i==4:\n",
        "      break\n",
        "    else:\n",
        "      first_verse.append(new_word)\n",
        "      cnn_embeddings = cnn.get_embedding(new_word)\n",
        "      phonological_embeddings = phn.get_embedding(urdu_to_roman[new_word]) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "      phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "      concat_vector.append(tf.concat([cnn_embeddings,phonological_embeddings], axis=0).numpy().tolist())\n",
        "  # At this point the concat_vector will contain the sentence from start to seed word in reverse order.\n",
        "  concat_vector.reverse()\n",
        "  first_verse.reverse()\n",
        "  # Getting next words\n",
        "  i=0\n",
        "  while(True):\n",
        "    new_word = bflm.predict(list(concat_vector))\n",
        "    i+=1\n",
        "    if new_word == '<EOS>' or i == 4:\n",
        "      break\n",
        "    else:\n",
        "      first_verse.append(new_word)\n",
        "      cnn_embeddings = cnn.get_embedding(new_word)\n",
        "      phonological_embeddings = phn.get_embedding(urdu_to_roman[new_word]) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "      phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "      concat_vector.append(tf.concat([cnn_embeddings,phonological_embeddings], axis=0))\n",
        "\n",
        "\n",
        "  verses.append(first_verse)\n",
        "\n",
        "for sub_words in range(len(word)):\n",
        "  print(word[sub_words])\n",
        "  print(verses[sub_words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3-IaNLZbVVW",
        "outputId": "9221af51-7938-47b7-c93d-e99683e4494a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 249ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "دوست\n",
            "['کلاہیں', 'ہشیاری', 'دل', 'دوست', 'کرتے', 'بناؤ', 'تنگ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phn = Phonological_embeddings()\n",
        "phonological_embeddings = phn.get_embedding(roman_word) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "concat_vector = [tf.concat([cnn_embeddings,phonological_embeddings], axis=0).numpy().tolist()]\n",
        "first_verse = [seed_word]"
      ],
      "metadata": {
        "id": "U-dZlKePbvf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(concat_vector[0]))"
      ],
      "metadata": {
        "id": "aKwizRrTcXwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8cd290-f72a-48bb-8fd7-86e79e4bbffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting previous words\n",
        "i = 0\n",
        "while(True):\n",
        "  new_word = bflm.predict(list(concat_vector), backward=True)\n",
        "  i+=1\n",
        "  if new_word == '<SOS>' or i==5:\n",
        "    break\n",
        "  else:\n",
        "    first_verse.append(new_word)\n",
        "    cnn_embeddings = cnn.get_embedding(new_word)\n",
        "    phonological_embeddings = phn.get_embedding(urdu_to_roman[new_word]) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "    phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "    concat_vector.append(tf.concat([cnn_embeddings,phonological_embeddings], axis=0).numpy().tolist())\n",
        "# At this point the concat_vector will contain the sentence from start to seed word in reverse order.\n",
        "concat_vector.reverse()\n",
        "first_verse.reverse()\n",
        "# Getting next words\n",
        "i=0\n",
        "while(True):\n",
        "  new_word = bflm.predict(list(concat_vector))\n",
        "  i+=1\n",
        "  if new_word == '<EOS>' or i == 5:\n",
        "    break\n",
        "  else:\n",
        "    first_verse.append(new_word)\n",
        "    cnn_embeddings = cnn.get_embedding(new_word)\n",
        "    phonological_embeddings = phn.get_embedding(urdu_to_roman[new_word]) # Expecting this fn to be part of the class and should return embeddings in (5,300) form\n",
        "    phonological_embeddings = add_subwords_together(phonological_embeddings)\n",
        "    concat_vector.append(tf.concat([cnn_embeddings,phonological_embeddings], axis=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "CQrz4ONNsbL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b42549b-7054-4f23-e8db-df28ec894d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 322ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "نشہ"
      ],
      "metadata": {
        "id": "WetLPKdAxd5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "Ph4GRslEnq5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dcf768-28e0-4c0a-8c35-96fafbe347a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "D6FjCfp4jBua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49c7d1b-30dd-43c6-ec66-abd2329f6a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "کمال"
      ],
      "metadata": {
        "id": "97arH3ayxZ21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "NXAEDtHDumLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58cefee-7aa7-4b19-815e-3c9d2ac71607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دِل"
      ],
      "metadata": {
        "id": "P-LiDMKQxQpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "y4lSIDYIxPuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735a4b41-18ba-48e7-900b-ca13bd6d66f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "زخم"
      ],
      "metadata": {
        "id": "n9p_j75X0Nb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "n6oqfsMy0Ozx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1e29db-631e-45f2-85ca-bd92cb05a5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "0FWQ6fBH8WrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd7ea52-a346-4e17-ee2f-73bf9ad0ee0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "FmnB0WjaHn9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5276136d-8f6b-4f73-a8be-091f72d15dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_verse"
      ],
      "metadata": {
        "id": "xpjUkMMEKm7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81feda7c-66ac-4192-b6c8-ceea713a3ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کے', 'مرے', 'میں', 'تر', 'دوست', 'شرر', 'دیکھنے', 'جانا', 'دیکھ']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(first_verse)"
      ],
      "metadata": {
        "id": "JUmUT1KM9CCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9342eb95-0fd6-4c22-e829-e2986af473b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'کے مرے میں تر دوست شرر دیکھنے جانا دیکھ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}