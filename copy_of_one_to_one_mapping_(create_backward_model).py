# -*- coding: utf-8 -*-
"""Copy of One to One Mapping (Create backward Model)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WpQ-rYNA369kfrBmoFUn85WQn3TokS_A

**Download urdu hack for preprocessing**
"""

!pip install urduhack

"""**Necassay Imports**"""

!pip install --upgrade click typeguard

import ast
import keras
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import models
from keras import layers
from keras.utils import plot_model
from google.colab import drive
from urduhack.normalization import normalize_characters

import numpy as np

drive.mount("/content/drive/")

df = pd.read_csv('/content/drive/MyDrive/GhazalsR5c.csv')
df

df['GhazalsU'] = df['GhazalsU'].apply(lambda x: ast.literal_eval(x))
# type(df['Ghalib Ghazals'].iloc[0])
poetry_data = list(df['GhazalsU'])
print(poetry_data)

# df1["Ghalib Ghazals"] = df1["Ghalib Ghazals"].apply( lambda x : x.replace("!", ""))
df['GhazalsR'] = df['GhazalsR'].apply(lambda x: ast.literal_eval(x))
# type(df['Ghalib Ghazals'].iloc[0])
poetry_data1 = list(df['GhazalsR'])
print(poetry_data1)

count=0
for poem in poetry_data:
  for misraa in poem:
    for words in misraa:
      count+=1
count

def preprocessing(poetry_list):
  resultant_list = []
  for poetry in poetry_list:
    misraa_list = []
    for misraa in poetry:
      tokenized = [normalize_characters(u'{}'.format(token)).replace('\u200c','').replace(' ','') for token in misraa]
      misraa_list.append(tokenized)
    resultant_list.append(misraa_list)
  return resultant_list

def preprocessing1(poetry_list):
  resultant_list = []
  for poetry in poetry_list:
    misraa_list = []
    for misraa in poetry:
      tokenized = misraa.split()

      misraa_list.append(tokenized)
    resultant_list.append(misraa_list)
  return resultant_list



data = preprocessing(poetry_data)
print(data)

data1 = poetry_data1
print(data1)

"""checking"""

s1=preprocessing(poetry_data)

for poetry in range(len(s1)):
  for misraa in range(len(s1[poetry])):
    print(s1[poetry][misraa])

"""checking

**To match the count of misraas in urdu and roman**
"""

for poetry in range(len(data)):
    if len(data[poetry])<len(data1[poetry]):
      extra_misraa = len(data1[poetry])-len(data[poetry])
      for i in range(extra_misraa):
        data1[poetry].remove(data1[poetry][len(data[poetry])])
      # print(len(data[poetry]))
      # print(len(data1[poetry]))
    elif len(data1[poetry])<len(data[poetry]):
      extra_misraa = len(data[poetry])-len(data1[poetry])
      for i in range(extra_misraa):
        data[poetry].remove(data[poetry][len(data1[poetry])])

"""**Check length of data is same**"""

count_a=0
count_b=0

for poetry in range(len(data)):
  # for misraa in range(len(data[poetry])):
    if(len(data[poetry]) == len(data1[poetry])):
      count_a=count_a+1
    else:
      count_b=count_b+1
      print([poetry])
      # print(len(data1[poetry]))
print(count_a)
print(count_b)

count_a=0
count_b=0

for poetry in range(len(data)):
   for misraa in range(len(data[poetry])):
    if(len(data[poetry][misraa]) == len(data1[poetry][misraa])):
      # print(data[poetry][misraa])
      # print(data1[poetry][misraa])
      count_a=count_a+1
    else:
      count_b=count_b+1
      print(poetry)
      print(data1[poetry][misraa])
      print(data[poetry][misraa])

print(count_a)
print(count_b)

data1[2][0]

"""**Check type of Data**"""

print(type(data))
print(data)
print(type(data[0]))
print(data[0])
print(type(data[0][0]))
print(data[0][0])
print(type(data[0][0][0]))
print(data[0][0][0])

"""**Rule 1 for mapping**"""

# var=["tum","dard-o-alam","app"]
# for words in range(len(var)):
#   if(var[words].find("-o-")!=-1):
#     a=var[words].index("-o-")
#     split_a=var[words][0:a]
#     split_b=var[words][a-1]+"o"
#     split_c=var[words][a+3:len(var[words])]
#     var[words]=split_a
#     var.insert(words+1,split_b)
#     var.insert(words+2,split_c)

# print(var)

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("-o-")!=-1):
          a=data1[poetry][misraa][words].index("-o-")
          split_a=data1[poetry][misraa][words][0:a]
          split_b=data1[poetry][misraa][words][a-1]+"o"
          split_c=data1[poetry][misraa][words][a+3:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          data1[poetry][misraa].insert(words+2,split_c)
        words=words+1

"""**RULE 2 FOR MAPPING**"""

var=["tum","dara-e-alam","app"]
for words in range(len(var)):
  if(var[words].find("a-e-")!=-1):
    a=var[words].index("a-e-")
    split_a=var[words][0:a+1]+'a'
    split_b=var[words][a+4:len(var[words])]
    var[words]=split_a
    var.insert(words+1,split_b)

print(var)

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("a-e-")!=-1):
          a=data1[poetry][misraa][words].index("a-e-")
          split_a=data1[poetry][misraa][words][0:a+1]+'a'
          split_b=data1[poetry][misraa][words][a+4:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
        words=words+1

"""**RULE 3**"""

words=0
var=["tum","dark-e-alam-e-shifa","app-e-ka"]
while words in range(len(var)):
  if(var[words].find("-e-")!=-1):
    a=var[words].index("-e-")
    split_a=var[words][0:a]+var[words][a-1]+var[words][a+1]
    split_b=var[words][a+3:len(var[words])]
    var[words]=split_a
    var.insert(words+1,split_b)
  words=words+1

print(var)

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("-e-")!=-1):
          a=data1[poetry][misraa][words].index("-e-")
          split_a=data1[poetry][misraa][words][0:a]+data1[poetry][misraa][words][a-1]+data1[poetry][misraa][words][a+1]
          split_b=data1[poetry][misraa][words][a+3:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
        words=words+1

"""**RULE 4**"""

words=0
var=["tum","dar-shifa","app-ka","jalal"]
while words in range(len(var)):
  if(var[words].find("-")!=-1):
    a=var[words].index("-")
    split_a=var[words][0:a]
    split_b=var[words][a+1:len(var[words])]
    var[words]=split_a
    var.insert(words+1,split_b)
  words=words+1

print(var)

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("-")!=-1):
          a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:a]
          split_b=data1[poetry][misraa][words][a+1:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
        words=words+1

"""RULE 5"""

words=0
var=["tum","bewaga","banage","jalal"]
while words in range(len(var)):
  if(var[words].find("bewa") !=-1):
    # a=var[words].index("bewa")
    split_a=var[words][0:2]
    split_b=var[words][2:len(var[words])]
    var[words]=split_a
    var.insert(words+1,split_b)
    words=words+1
  words=words+1

print(var)

var="ggfhh"
print(var.find("h"))
print(len(var))

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("ga") !=-1 and data1[poetry][misraa][words].find("ga") == len(data1[poetry][misraa][words])-2):
          # a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:len(data1[poetry][misraa][words])-2]
          split_b=data1[poetry][misraa][words][len(data1[poetry][misraa][words])-2:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          words=words+1
        words=words+1

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("ge") !=-1 and data1[poetry][misraa][words].find("ge") == len(data1[poetry][misraa][words])-2):
          # a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:len(data1[poetry][misraa][words])-2]
          split_b=data1[poetry][misraa][words][len(data1[poetry][misraa][words])-2:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          words=words+1
        words=words+1

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("gi") !=-1 and data1[poetry][misraa][words].find("gi") == len(data1[poetry][misraa][words])-2):
          # a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:len(data1[poetry][misraa][words])-2]
          split_b=data1[poetry][misraa][words][len(data1[poetry][misraa][words])-2:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          words=words+1
        words=words+1

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("bewa") !=-1):
          # a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:2]
          split_b=data1[poetry][misraa][words][2:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          words=words+1
        words=words+1

for poetry in range(len(data1)):
   for misraa in range(len(data1[poetry])):
    if(len(data[poetry][misraa]) != len(data1[poetry][misraa])):
      words=0
      while words in range(len(data1[poetry][misraa])):
        if(data1[poetry][misraa][words].find("rahg") !=-1):
          # a=data1[poetry][misraa][words].index("-")
          split_a=data1[poetry][misraa][words][0:3]
          split_b=data1[poetry][misraa][words][3:len(data1[poetry][misraa][words])]
          data1[poetry][misraa][words]=split_a
          data1[poetry][misraa].insert(words+1,split_b)
          words=words+1
        words=words+1

import pickle
pickle.dump(data1,open("mapped_to_urdu.csv","wb"))

"""**Embeddings**"""

splits=['e', 'o', 'a', 'i', 'u' ];
splits_b=['aa','ii','uu'];
splits_c=['e', 'o', 'i', 'u' ];
consonants=['ch','kh','sh','gh','bh','ph','th','jh','Th','chh','dh','ddh','rhh','b','p','t','j','s','d','z','r','f','k','g','l','m','n','h','w']

"""**Function to get phonetics**"""

def return_subwords(token):
  a = []
  j = 0

  if len(token) <= 3:
    a.append([token])

  elif len(token) == 4:
    for i in range (0, len(token)-1):
      if(token[i] in consonants and token[i+1] in consonants):
        if(len(token[j:i+1])>1):
            a.append([token[j:i+1]])
            j=i+1

      elif(token[i] in splits and token[i+1] in consonants):
        if(len(token[j:i+1])>1):
            a.append([token[j:i+1]])
            j=i+1

      else:
         a.append([token[j:len(token)]])
         j=i+2

  else:
    for i in range(0, len(token)):
      if token[i:i+3] in consonants and i+3 < len(token):
        if i+3 == len(token)-1:
          if(len(token[j:i+4])>1):
            a.append([token[j:i+4]])
            j=i+4

        else:
          if(len(token[j:i+5])>1):
            a.append([token[j:i+5]])
            j=i+5

      elif token[i:i+2] in splits_b and i+2 < len(token):
        if token[i+2] in consonants and i+2 == len(token)-1:
          if(len(token[j:i+3])>1):
            a.append([token[j:i+3]])
            j=i+3

        else:
          if(len(token[j:i+2])>1):
            a.append([token[j:i+2]])
            j=i+2

      elif token[i] in splits:
          if token[i+1:i+4] in consonants and i+5 == len(token):
            if(len(token[j:i+1])>1):
              a.append([token[j:i+1]])
              j=i+1

          elif token[i+1:i+3] in consonants:
            if(len(token[j:i+3])>1):
              a.append([token[j:i+3]])
              j=i+3

          elif token[i+1:i+2] in consonants and token[i+2:i+3] in consonants:
            if(len(token[j:i+1])>1):
              a.append([token[j:i+2]])
              j=i+2

          elif i+2 == len(token):
            if(len(token[j:i+2])>1):
              a.append([token[j:i+2]])
              j=i+2

          else:
            if(len(token[j:i+1])>1):
              a.append([token[j:i+1]])
              j=i+1

  a = [_ for __ in a for _ in __]
  return a

"""**Function to get prosody**"""

def calculate_prosody(token):
  n = []
  b = []

  for i in range(0, len(token)):
    if len(token[i]) >= 3:
      n.append(3)
      b.append(110)

    elif len(token[i]) == 1:
      n.append(2)
      b.append(10)

    else:
      if token[i][0:2] in splits_b:
        n.append(3)
        b.append(110)

      elif ((token[i][0:1] in splits and token[i][1:2] in consonants) or (token[i][1:2] in splits and token[i][0:1] in consonants)) and i < len(token)-1:
        n.append(3)
        b.append(110)

      else:
        n.append(2)
        b.append(10)

  return [n, b]

data1[0]

"""**Driver program**"""

# text ="sitāroñ se aage jahāñ aur bhī haiñ abhī ishq ke imtihāñ aur bhī haiñ tahī zindagī se nahīñ ye fazā.eñ yahāñ saikḌoñ kārvāñ aur bhī haiñ qanā.at na kar ālam-e-rañg-o-bū par chaman aur bhī āshiyāñ aur bhī haiñ agar kho gayā ik nasheman to kyā ġham maqāmāt-e-āh-o-fuġhāñ aur bhī haiñ tū shāhīñ hai parvāz hai kaam terā tire sāmne āsmāñ aur bhī haiñ isī roz o shab meñ ulajh kar na rah jā ki tere zamān o makāñ aur bhī haiñ ga.e din ki tanhā thā maiñ anjuman meñ yahāñ ab mire rāz-dāñ aur bhī haiñ"
# text = "Thikaanaa"
# text = "khaak"
# text = "meiN"
# text = "khushbu"
# text = "sakte"
# text = "ki daanaa Khaak meiN mil kar gul-e-gulzaar hotaa hai"
# text = "sapne"
# text = "Sare sapne kahin kho gaye"
# text = "sitaron se aage jahan aur bhi haiN"
# text = "KL SAHL DREA PH KHA MJH SE KHZR NE TO DHOND RHA HE SM AFRNG KA TREAK؟ AK NKTH MRE PAS HE SHMSHER KE MANND BRNDH O SEKL ZDH O ROSHN O BRAK KAFR KE EH PHCHAN KH AAFAK MEN GM HE MOMN KE EH PHCHAN KH GM AS MEN HEN AAFAK"

splits=['e', 'o', 'a', 'i', 'u' ];
splits_b=['aa','ii','uu'];
splits_c=['e', 'o', 'i', 'u' ];
consonants=['ch','kh','sh','gh','bh','ph','th','jh','Th','chh','dh','ddh','rhh','b','p','t','j','s','d','z','r','f','k','g','l','m','n','h','w']

arr = [];
result = []
num_pros = []
bin_pros = []

for poem in data1:
  ar = []
  nr = []
  br = []

  for tokens in poem:
    a = []
    n = []
    b = []

    for token in tokens:
      a.append(return_subwords(token))
      n.append(calculate_prosody(a[-1])[0])
      b.append(calculate_prosody(a[-1])[1])

    ar.append(a)
    nr.append(n)
    br.append(b)

  arr.append(ar)
  num_pros.append(nr)
  bin_pros.append(br)

print(type(arr))
print(type(num_pros))
print(type(bin_pros))



"""**Encoding**"""

from sklearn.preprocessing import LabelEncoder

flattened_l = [e for sublist in arr for e in sublist]
flattened_l = [e for sublist in flattened_l for e in sublist]
flattened_l = [e for sublist in flattened_l for e in sublist]

# flattened_l is ['PER', 'O', 'O', 'GEO', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'O', 'PER', 'O']

le = LabelEncoder().fit(flattened_l)

# See the mapping generated by the encoder:
dat = list(enumerate(le.classes_))

wordDict = {}
res = []

for i in range(0, len(dat)):
  wordDict[i] = dat[i]

for poem in arr:
  ar = []

  for tokens in poem:
    ab = []

    for token in tokens:
      a = []

      for t in token:

        for j in range (0, len(wordDict)):
          if t == wordDict[j][1]:
            a.append(wordDict[j][0])

      ab.append(a)

    ar.append(ab)

  res.append(ar)

res

len(wordDict)

"""**Max Length**"""

maxlen = 0

for poem in arr:

  for tokens in poem:

    for token in tokens:
        if len(token) > maxlen:
          print(token)
          maxlen = len(token)

maxlen

"""**Padding**"""

for poem in res:

  for tokens in poem:

    for token in tokens:
      if len(token) < maxlen:
        token += [len(wordDict)+1]*(maxlen-len(token))

"""**Padding for prosody**"""

for bins in num_pros:

  for bin in bins:

    for b in bin:
      if len(b) < maxlen:
        b += [0]*(maxlen-len(b))

"""
**Padding for words in misra**"""

max=0;
for poem in res:
  for misraa in poem:
      if len(misraa)>max:
        max=len(misraa);
max

for poem in res:
  for misraa in poem:
    for i in range(max-len(misraa)):
      misraa.append([len(wordDict)+1,len(wordDict)+1,len(wordDict)+1,len(wordDict)+1,len(wordDict)+1])

"""**Padding for misra in poems (prosody)**"""

maxx=0;
for poem in num_pros:
  for misraa in poem:
      if len(misraa)>maxx:
        maxx=len(misraa);
maxx

for poem in num_pros:
  for misraa in poem:
    for i in range(max-len(misraa)):
      misraa.append([0,0,0,0,0])

min=100000;
for poem in res:
      if len(poem)<min:
        min=len(poem);
min

for i in range(len(res)):
    res[i] = res[i][:min]  # Truncate each poem to the first 100 characters

min=100000;
for poem in num_pros:
      if len(poem)<min:
        min=len(poem);
min

for i in range(len(num_pros)):
    num_pros[i] = num_pros[i][:min]  # Truncate each poem to the first 100 characters

import numpy as np
import tensorflow as tf

x = np.array(res)
y = np.asarray(num_pros)

"""**Import Libraries**"""

!pip install tensorflow

"""**Model Initialization**"""

import keras

from keras.models import Sequential
from keras.layers import Dense, Embedding, GRU, Dropout, Bidirectional, SpatialDropout1D,TimeDistributed,LSTM

EMBEDDING_DIM = 300
vocab_size = list(wordDict.keys())[-1] + 2

def return_model():
  model = Sequential()

  model.add(Embedding(vocab_size, EMBEDDING_DIM))
  model.add(Bidirectional(GRU(256, return_sequences=True)))

  model.add(Dropout(0.2))

  model.add(TimeDistributed(Dense(4, activation='softmax')))

  model.compile(loss='categorical_crossentropy' , metrics=['accuracy'], optimizer=keras.optimizers.Adam())

  return model

"""**Formatting the list**"""

x_train = []
y_train = []

for poems in x:
  for poem in poems:
     x_train.append(poem)

for poems in y:
  for poem in poems:
     y_train.append(poem)

type(x_train)

x_train = [x_ for x in x_train for x_ in x]
y_train = [y_ for y in y_train for y_ in y]

x_train[0]

x_train = np.array(x_train).astype(np.float32)
y_train = np.array(y_train).astype(np.float32)

x_train[0]

y_train = tf.keras.utils.to_categorical(y_train, 4)

y_train.shape

"""**Bi-GRU for embedding**"""

from keras.models import Model
from keras.layers import Input, Embedding, Bidirectional, GRU, Dropout, TimeDistributed, Dense

EMBEDDING_DIM = 300
vocab_size = list(wordDict.keys())[-1] + 2

def return_model():
    inputs = Input(shape=(x_train.shape[1],))
    x = Embedding(vocab_size, EMBEDDING_DIM)(inputs)
    x = Bidirectional(GRU(256, return_sequences=True))(x)
    x = Dropout(0.2)(x)
    outputs = TimeDistributed(Dense(4, activation='softmax'))(x)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
    return model

model = return_model()
model.summary()

model.fit(x_train, y_train, epochs=2)

# Extract output from the embedding layer (first layer)
intermediate_layer_model = Model(inputs=model.input,
                                 outputs=model.get_layer(index=1).output)

intermediate_output = intermediate_layer_model.predict(x_train)

intermediate_layer_model = keras.Model(inputs=model.input,
                                       outputs=model.layers[0].output)
intermediate_output = intermediate_layer_model(x_train)
intermediate_output

intermediate_output.shape

model.save('/content/drive/MyDrive/Datasets/backward_model2q.h5')

